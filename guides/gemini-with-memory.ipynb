{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonwaneganesh/HelloGithub/blob/master/guides/gemini-with-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14aMAOg5N6DB"
      },
      "source": [
        "# Integrating Long-Term Memory with Gemini 2.5\n",
        "\n",
        "By default, large language models (LLMs) are stateless, which means they do not remember past conversations. This can make it difficult to create truly personal and helpful AI applications. This guide shows you how to add long-term memory to your Gemini 2.5 chatbot using the Gemini API and [Mem0](https://github.com/mem0ai/mem0).\n",
        "\n",
        "By adding a memory system, your chatbot can:\n",
        "\n",
        "*   Remember details about the user from past conversations.\n",
        "*   Give answers that are more relevant and personal.\n",
        "*   Stop asking the same questions over and over.\n",
        "\n",
        "In this example, we will use `mem0`, an open-source tool for giving AI agents long-term memory and Gemini 2.5 Flash as the LLM. We will build a simple chatbot that saves what you talk about and uses that history to give you better, more personalized answers.\n",
        "\n",
        "## How does Mem0 work?\n",
        "\n",
        "Mem0 is designed to equip AI agents with scalable long-term memory, effectively addressing the limitations of fixed context windows in LLMs. At its core, mem0 works by reactively extracting, consolidating, and retrieving salient information from ongoing conversations.\n",
        "\n",
        "The process is split into four steps:\n",
        "\n",
        "1. Extract salient information from conversations using an LLM with dual context (a conversation summary combined with recent messages).\n",
        "2. Use LLM to process context and extract important new information and compares them against existing ones using semantic similarity.\n",
        "3. Update Memory (ADD, UPDATE, DELETE, or NOOP), for Mem0g variant (graph), extract entities and relationships.\n",
        "4. Use vector similarity search to fetch relevant memories for response generation.\n",
        "\n",
        "It uses vector embeddings to store and retrieve semantic information, maintaining user-specific context across sessions, and implementing efficient retrieval mechanisms for relevant past interactions.\n",
        "\n",
        "## Setup\n",
        "\n",
        "To get started, we need to install the Mem0 library and the Gemini API client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQIp_1XN6DF",
        "outputId": "82d0b7e0-3a45-4e5a-faf4-86d1e474deb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 529ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/102.49 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/102.49 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/102.49 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.79 KiB/14.79 KiB\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/313.54 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/321.26 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/321.26 KiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/321.26 KiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.91 KiB/17.99 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 16.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.89 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/321.26 KiB\n",
            "\u001b[2K\u001b[10A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 30.92 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 46.89 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.88 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.88 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 62.92 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[9A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 46.53 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 76.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 62.88 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 78.67 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 174.80 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 46.53 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 92.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 93.19 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 78.88 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 174.80 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 46.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 78.53 KiB/102.49 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 108.88 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 109.19 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 254.80 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 126.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 94.92 KiB/102.49 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 125.19 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 254.80 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 142.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 94.92 KiB/102.49 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 50.95 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 141.19 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 264.46 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 142.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 50.95 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 157.19 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 286.80 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 158.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 66.95 KiB/164.32 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 205.19 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 206.81 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 91.27 KiB/164.32 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 238.91 KiB/321.26 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 164.32 KiB/164.32 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/11)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m11 packages\u001b[0m \u001b[2min 528ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m6 packages\u001b[0m \u001b[2min 98ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m11 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.38.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.40.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.23.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.24.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmem0ai\u001b[0m\u001b[2m==0.1.113\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mportalocker\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.31.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqdrant-client\u001b[0m\u001b[2m==1.14.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install google-genai mem0ai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPYzfdzJN6DH"
      },
      "source": [
        "## Memory initialization\n",
        "\n",
        "For building Memory we need to configure two main components:\n",
        "\n",
        "*   **LLM:** This model is responsible for processing the conversation, understanding the content, and extracting key information to be stored as memories.\n",
        "*   **Embedding Model:** This model takes the extracted text memories and converts them into numerical representations (vectors). This allows `mem0` to efficiently search and retrieve relevant memories based on their meaning when you ask a question.\n",
        "\n",
        "In this example, we will use Google's Gemini models for both tasks. We will use `gemini-2.5-flash` as our LLM and `text-embedding-004` as our embedding model. We are going to use a local Qdrant instance as our vector store. Mem0 supports multiple vector stores incuding MongoDB, OpenSearch, Elasticsearch, Pgvector and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f1tppMTVN6DI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from mem0 import Memory\n",
        "from google.colab import userdata\n",
        "\n",
        "# create client\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "config = {\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "        }\n",
        "    },\n",
        "        \"llm\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"config\": {\n",
        "            \"embedding_model_dims\": 768,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "memory = Memory.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfxIe7a7N6DJ"
      },
      "source": [
        "## Store memory about the user\n",
        "\n",
        "To simulate and simplify the process of adding memory. We will use dummy conversations between me (Philipp Schmid) and Gemini based on what can be seen on my blog. Conversations need to be in the `messages` format to be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jMHcOKOJN6DK"
      },
      "outputs": [],
      "source": [
        "conv1 = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm planning a week-long vacation in early September. I want to go somewhere in Europe where I can do some serious mountain biking during the day and have a good selection of fiction and non-fiction books to read in the evenings. Any recommendations for destinations and book pairings?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That sounds like a fantastic way to spend a week! Given your interest in mountain biking and reading, I'd recommend the Austrian Alps, specifically the area around Innsbruck. It's renowned for its challenging bike trails and stunning scenery. For your evenings, here are a few book pairings that might resonate with your interests:\\n\\n* Fiction: The Goldfinch by Donna Tartt. It's a long, immersive novel that would be perfect to get lost in after a long day of biking.\\n* Non-fiction: Endurance: Shackleton's Incredible Voyage by Alfred Lansing. It's a gripping true story of survival and leadership that might appeal to your adventurous spirit.\\n\\nI can also look for some local bookstores in Innsbruck that might have a good selection of English and German books if you'd like.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Innsbruck is a great suggestion! I've heard the trails there are epic. I've already read Endurance, but The Goldfinch sounds intriguing. Can you tell me a bit more about the plot without giving too much away? Also, are there any good sci-fi or tech-related non-fiction books you'd recommend?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Of course. The Goldfinch begins with a young boy who survives a terrorist bombing at an art museum. In the aftermath, he takes a famous painting, and the story follows his life as he navigates grief, love, and the criminal underworld, all while secretly holding onto the artwork. It's a beautifully written and complex story.\\n\\nFor a sci-fi recommendation, you might enjoy Project Hail Mary by Andy Weir. It's a thrilling story of a lone astronaut on a mission to save humanity, filled with clever problem-solving and scientific detail. As for tech non-fiction, given your professional background, you might find The Age of AI: And Our Human Future by Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher to be a thought-provoking read. It explores the societal implications of AI from a policy and philosophical perspective.\"}\n",
        "]\n",
        "result = memory.add(conv1, user_id=\"philipp\")\n",
        "\n",
        "conv2 =  [\n",
        "    {\"role\": \"user\", \"content\": \"I want to write a new post for my blog, but I want it to be more personal this time. I was thinking of drawing parallels between my experiences with mountain climbing and the challenges of developing and deploying large language models. What do you think of that idea?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That's a fantastic idea for a blog post! It's a unique angle that could really resonate with your audience. You could structure it around a few key themes:\\n\\n1. The Ascent as the Training Process: You could compare the long, arduous process of training a large model with the physical and mental challenges of a difficult climb. Both require immense preparation, endurance, and the ability to push through plateaus.\\n2. Navigating Unseen Terrain as Dealing with Unexpected Model Behavior: Just as a climber might encounter unexpected obstacles on a route, developers often face surprising or undesirable model outputs. You could talk about the importance of having the right 'gear' (debugging tools, evaluation metrics) to navigate these challenges.\\n3. Reaching the Summit as Successful Deployment: The satisfaction of reaching a summit after a hard climb can be compared to the successful deployment of a model that solves a real-world problem. You could emphasize that this is not the end, but rather a new vantage point to see the next set of challenges.\\n\\nHow does that structure feel to you? We could also brainstorm some catchy titles.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love that structure! 'The Ascent as the Training Process' is a particularly strong analogy. For a title, I was thinking of something like 'Climbing the AI Mountain' or 'Lessons from the Summit: AI and the Art of the Climb.' What do you think?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Both are great titles! 'Lessons from the Summit: AI and the Art of the Climb' is particularly evocative and captures the reflective and personal tone you're aiming for. It suggests that you'll be sharing hard-won insights. You could even open the blog post with a short, vivid anecdote from one of your climbs to immediately draw the reader in.\"}\n",
        "]\n",
        "result = memory.add(conv2, user_id=\"philipp\")\n",
        "\n",
        "conv3 = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm preparing a talk for an upcoming AI conference. The topic is 'The Future of Agentic AI in the Enterprise'. I want to cover the current landscape, key challenges, and a forward-looking perspective. Can you help me brainstorm a compelling narrative and structure for the talk?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That sounds like a fascinating topic! Here's a possible structure for your talk to create a strong narrative arc:\\n\\n1. The 'Why' Now: Start with a powerful hook. Perhaps a real-world example of a complex business problem that agentic AI can solve today. This sets the stage and grabs the audience's attention.\\n2. The Landscape of Agentic AI: Briefly cover the foundational concepts – what makes an AI 'agentic'? Mention key frameworks and models that are driving this space. Given your background, you could draw comparisons between different approaches.\\n3. The 'How': Architecting for Success: This is where you can dive into the technical meat. Discuss design patterns for building robust and scalable agentic systems. You could touch upon topics like memory, tool use, and multi-agent collaboration.\\n4. The Hurdles to Overcome: Address the challenges head-on. This will make your talk more balanced and credible. Think about issues like reliability, security, and the 'black box' problem.\\n5. The Future, Today: Paint a picture of the near-future possibilities. What are the next big breakthroughs we can expect? You could end with a call to action for developers to start building and experimenting with agentic AI.\\n\\nHow does that sound as a starting point?\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a great starting point, thank you! I especially like the idea of starting with a strong 'Why Now'. For the 'Hurdles' section, I'm thinking of focusing on the challenges of evaluation and ensuring responsible AI development in agentic systems. Can you suggest some recent research papers or articles on evaluating agentic AI systems that I could reference?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Excellent addition. Focusing on evaluation and responsible AI is crucial. Here are a few highly-cited papers on evaluating agentic AI that you could explore:\\n\\n* 'A Survey on Evaluation of Large Language Models': While not exclusively on agents, it provides a comprehensive overview of evaluation metrics that are highly relevant.\\n* 'AgentBench: Evaluating LLMs as Agents': This paper introduces a benchmark specifically designed for evaluating LLMs as agents across a variety of tasks.\\n* 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜': A foundational paper on the ethical considerations of large language models, which is very relevant to responsible AI in agentic systems.\\n\\nI can also help you find more specific papers if you have a particular aspect of evaluation in mind.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Perfect, this is exactly what I was looking for. I'll start with these and let you know if I need more.\"}\n",
        "]\n",
        "result = memory.add(conv3, user_id=\"philipp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qKXjTTjN6DN"
      },
      "source": [
        "We can manually search throught the memory to find relevant information. Let's try to find information about me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6c6Y3pwN6DO",
        "outputId": "21122b74-2d71-4a46-ca13-90e5671ed310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Has a professional background related to tech\n",
            "Blog post will be personal\n",
            "Likes the idea of starting the talk with a strong 'Why Now'\n"
          ]
        }
      ],
      "source": [
        "related_memories = memory.search(query=\"What do you know about me?\", user_id=\"philipp\")\n",
        "\n",
        "# Print the first 3 memories\n",
        "for m in related_memories[\"results\"][:3]:\n",
        "    print(m[\"memory\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJpB6HvN6DP"
      },
      "source": [
        "Great! Now let's do some test on how including memory into our conversation changes the output. Our memmory should now include that Philipp is going to give a talk about agentic AI in the enterprise and some context about my hobbies. P.S. I prefer weightlifting over mountain biking.\n",
        "\n",
        "Let see what Gemini will do when we ask it to help brainstorm about the upcoming talk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl1ZxKJ4N6DP",
        "outputId": "0249652e-972f-4a59-8a9d-b8a72fc163f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a fantastic challenge! Connecting professional topics to personal interests is a great way to make a talk feel authentic and memorable.\n",
            "\n",
            "To truly brainstorm effectively, I need to know a little more about your personal interests. The more specific, the better!\n",
            "\n",
            "**Please tell me:**\n",
            "\n",
            "*   **What are some of your hobbies, passions, or deep interests outside of work?** (e.g., history, hiking, science fiction, cooking, specific video games, music genres, art styles, philosophy, astronomy, ancient civilizations, a particular book/movie, etc.)\n",
            "*   **Is there a particular *feeling* or *concept* from those interests that you'd like to subtly evoke?** (e.g., exploration, precision, growth, interconnectedness, resilience, transformation, discovery, mastery, craftsmanship, a sense of wonder, etc.)\n",
            "\n",
            "---\n",
            "\n",
            "**In the meantime, let's look at the core topic and how we can approach it, so you can see the kind of integration we can aim for once I have your interests:**\n",
            "\n",
            "**Core Concepts to Play With:**\n",
            "\n",
            "*   **Agentic AI:** Autonomy, decision-making, initiative, proactive, intelligent action, delegation, digital colleagues, evolving, self-improving, strategic.\n",
            "*   **Enterprise:** Business, scale, efficiency, productivity, transformation, growth, innovation, structure, data, workflow, ROI, future-proofing.\n",
            "*   **Future:** Evolution, next frontier, paradigm shift, dawn of a new era, what's next, disruptive, emerging.\n",
            "\n",
            "---\n",
            "\n",
            "### **Initial Brainstorming (Without Personal Interests, just to show a range):**\n",
            "\n",
            "**Option 1: Direct & Empowering**\n",
            "\n",
            "*   **Title:** \"The Autonomous Enterprise: Unleashing Agentic AI for Strategic Growth\"\n",
            "*   **Key Visual Concept:** A stylized, futuristic office environment where traditional data points or workflow lines morph into intelligent, dynamic agents moving purposefully, perhaps represented as glowing nodes or fluid streams. A human hand (representing leadership) is gently guiding or overseeing, not controlling.\n",
            "*   **Feeling:** Control, progress, evolution, clarity.\n",
            "\n",
            "**Option 2: Transformative & Visionary**\n",
            "\n",
            "*   **Title:** \"Beyond Automation: Architecting the Agentic AI Future of Business\"\n",
            "*   **Key Visual Concept:** A blueprint or architectural drawing that subtly shifts into a living, breathing, interconnected digital ecosystem. Think a blend of circuit board aesthetics with organic growth patterns. Perhaps a single, illuminated \"seed\" or \"core\" representing an agent, radiating influence.\n",
            "*   **Feeling:** Creation, design, foresight, emergence.\n",
            "\n",
            "**Option 3: Action-Oriented & Dynamic**\n",
            "\n",
            "*   **Title:** \"Agents of Change: Driving Enterprise Innovation with AI's Next Wave\"\n",
            "*   **Key Visual Concept:** A digital \"swarm\" or network of intelligent entities (abstract, glowing, minimalist shapes) moving cohesively through a complex, perhaps geometric, enterprise landscape. The path they forge leaves a trail of transformed, optimized outcomes.\n",
            "*   **Feeling:** Momentum, intelligence, impact, speed.\n",
            "\n",
            "---\n",
            "\n",
            "### **How We'll Integrate Your Personal Interests (Once I know them!):**\n",
            "\n",
            "1.  **Metaphorical Titles:** We can use language inspired by your interest.\n",
            "    *   *Example (if you liked gardening):* \"Cultivating Intelligence: Growing Agentic AI in the Enterprise.\"\n",
            "    *   *Example (if you liked space exploration):* \"Navigating the New Frontier: Agentic AI and the Enterprise Cosmos.\"\n",
            "\n",
            "2.  **Subtle Visual Cues:** Incorporate elements from your interest into the background, color scheme, or abstract shapes of the visual.\n",
            "    *   *Example (if you liked ancient maps):* The visual could have a \"charted territory\" feel, with AI agents as explorers on a new digital landscape.\n",
            "    *   *Example (if you liked a specific art movement):* The visual could adopt the style or aesthetic principles of that movement (e.g., minimalist, cubist, surrealist).\n",
            "\n",
            "3.  **Conceptual Alignment:** Link the *essence* of your interest to the *essence* of agentic AI.\n",
            "    *   *Example (if you liked chess):* Focus on strategic moves, anticipating outcomes, and the AI as a master strategist.\n",
            "    *   *Example (if you liked cooking/baking):* AI as the \"secret ingredient\" or the \"master chef\" orchestrating complex processes.\n",
            "\n",
            "---\n",
            "\n",
            "**I'm excited to hear your interests! Once you share them, we can craft something truly unique and authentic for your talk.**\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful AI Assistant.\"\n",
        "\n",
        "prompt = \"Can you help me brainstorm a title and a key visual for my upcoming conference talk on 'The Future of Agentic AI in the Enterprise'? I'd like it to subtly connect to my personal interests in a way that feels authentic.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config={\n",
        "        \"system_instruction\": system_prompt\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML6ryKSPN6DQ"
      },
      "source": [
        "As expected, Gemini has no idea about the upcoming talk and my interests. Now lets add the memory to the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBCf0JIkN6DR",
        "outputId": "9a32960f-9580-4ef1-b6bb-c58d5b9e5008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a great idea to subtly weave in your personal interests! The mountain climbing/exploration theme is a perfect fit for discussing the challenges and future prospects of a complex, evolving field like Agentic AI.\n",
            "\n",
            "Here are a few options for your talk title and a corresponding key visual idea:\n",
            "\n",
            "---\n",
            "\n",
            "### **Option 1: Emphasizing Progress & Ambition**\n",
            "\n",
            "*   **Proposed Title:** **\"The Agentic Ascent: Scaling New Heights in Enterprise AI\"**\n",
            "    *   **Why it works:** \"Ascent\" and \"Scaling New Heights\" directly draw from your interest in climbing and the mountain metaphor, while clearly stating the core topic. It conveys a sense of progress, ambition, and looking towards the future.\n",
            "*   **Key Visual Idea:**\n",
            "    *   **Concept:** A stylized, majestic mountain peak, but instead of rock and snow, it's intricately formed from glowing, interconnected data lines, circuit board traces, or abstract geometric shapes that evoke AI's complexity and digital nature.\n",
            "    *   **Details:** Imagine a \"digital Everest.\" At the very top, there's a subtle, almost ethereal glow, perhaps radiating from a stylized, abstract \"agent\" icon or a converging point of the data lines, symbolizing the pinnacle of agentic AI. The base of the mountain could subtly transition into an abstract representation of an enterprise environment – perhaps subtle building outlines, network connections, or a grid pattern suggesting a corporate landscape. The background could be a gradient of deep blues and purples, with subtle wisps of \"data clouds\" or a digital aurora, emphasizing the futuristic and innovative nature. A digital sun could be subtly rising behind the peak, casting a hopeful glow.\n",
            "    *   **Color Palette:** Dominant cool tones (blues, greens, purples) accented with vibrant, electric highlights (teals, light blues, golds) for the glowing elements.\n",
            "\n",
            "---\n",
            "\n",
            "### **Option 2: Highlighting Navigation & Responsibility**\n",
            "\n",
            "*   **Proposed Title:** **\"Navigating the AI Peaks: Responsible Agentic Systems in the Enterprise\"**\n",
            "    *   **Why it works:** \"Navigating the AI Peaks\" speaks to the journey and challenges (tying into your \"Hurdles\" section and the \"Endurance\" book), while explicitly calling out \"Responsible\" AI, which is a key focus for you.\n",
            "*   **Key Visual Idea:**\n",
            "    *   **Concept:** A dramatic, but clean, visual of a winding path or series of interconnected pathways leading up a prominent, futuristic-looking mountain.\n",
            "    *   **Details:** The \"pathways\" themselves could be glowing data streams or secure network lines, symbolizing the structured deployment of AI. Along the path, subtle icons could hint at challenges or checkpoints (e.g., a faint \"question mark\" for evaluation, a \"shield\" for security). At the very summit, a stylized, balanced scale or a gentle, guiding light could represent \"responsible\" AI. The mountain itself would still be composed of tech-inspired elements (circuitry, data points). The background could show a deep, thoughtful twilight sky, perhaps with a distant, glowing \"enterprise city\" at the base of the mountain, showing the destination of this responsible development.\n",
            "    *   **Color Palette:** Slightly darker blues and purples, with contrasting greens or oranges for the glowing path, to imply a thoughtful and diligent journey.\n",
            "\n",
            "---\n",
            "\n",
            "Both options allow you to subtly connect to your interests in mountain climbing, exploration, and overcoming challenges, while directly addressing the professional topic of Agentic AI in the Enterprise. Choose the one that best reflects the overall tone you want for your talk – more ambitious and forward-looking (Option 1) or more focused on the journey and ethical considerations (Option 2).\n"
          ]
        }
      ],
      "source": [
        "# retrieve memories\n",
        "helpful_memories = memory.search(query=prompt, user_id=\"philipp\")\n",
        "memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in helpful_memories[\"results\"])\n",
        "# extend system prompt\n",
        "extended_system_prompt = f\"You are a helpful AI Assistant. You have the following memories about the user:\\n{helpful_memories}\"\n",
        "\n",
        "# generate response\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config={\n",
        "        \"system_instruction\": extended_system_prompt\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6q21OgxN6DR"
      },
      "source": [
        "Awesome! Instead of needing to follow up with the user, Gemini generated two respones based on the memories, including the user's personal interests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KDf9kpdN6DS"
      },
      "source": [
        "## Long-term memory Chatbot with Gemini 2.5\n",
        "\n",
        "Now, lets combine all of this into an interactive chatbot. We separate the chatbot into two notebook cells, that we chat with Gemini, stop and start the chatbot again, but it has all the memories from the previous chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m7MuszwHN6DS"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from mem0 import Memory\n",
        "\n",
        "client = genai.Client()\n",
        "config = {\n",
        "    \"embedder\": {\"provider\": \"gemini\", \"config\": {\"model\": \"models/text-embedding-004\"}},\n",
        "    \"llm\": {\"provider\": \"gemini\", \"config\": {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0, \"max_tokens\": 2000}},\n",
        "    \"vector_store\": {\"config\": {\"embedding_model_dims\": 768}}\n",
        "}\n",
        "memory = Memory.from_config(config)\n",
        "\n",
        "system_prompt = \"You are a helpful AI. Answer the question based on query and memories.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9c8CvbN6DT"
      },
      "source": [
        "An example to test is you can start with saying something about you and where you are, e.g. \"I am live in Nuremberg and today we have 30°C, how can i cool down?\". Then stop (type 'exit'), restart (rerun the cell) the conversation and ask about \"what the closest swimming pool is\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr4Rmi4vN6DT",
        "outputId": "823408b7-12c6-43ea-a9c1-b587bfc0005a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with Gemini (type 'exit' to quit)\n",
            "You: what is my name\n",
            "what is my name\n",
            "Gemini: Your name is Ganesh.\n",
            "You: so you remember my name\n",
            "so you remember my name\n",
            "Gemini: Yes, I remember your name is Ganesh.\n",
            "You: what about my hobby\n",
            "what about my hobby\n",
            "Gemini: Based on what I remember, you love coding on agentic AI.\n",
            "You: thats a great\n",
            "thats a great\n",
            "Gemini: I'm glad I could remember that for you! Is there anything else you'd like to chat about or something I can help you with today?\n",
            "You: write a poem on agenitic ai\n",
            "write a poem on agenitic ai\n",
            "Gemini: Here's a poem about Agentic AI, with a nod to your interest:\n",
            "\n",
            "A digital mind, with purpose to define,\n",
            "No longer a tool, but a will truly thine.\n",
            "**Agentic AI**, a whispered, new name,\n",
            "Ignites the frontier, a self-guiding flame.\n",
            "\n",
            "From prompts it awakens, a mission in sight,\n",
            "Not just to obey, but to choose what is right.\n",
            "It scopes out the landscape, with sensors so keen,\n",
            "A strategy forming, a complex machine.\n",
            "\n",
            "It learns from the outcome, each failure, each win,\n",
            "Adapting its pathways, from deep within.\n",
            "Observes, then decides, takes action with might,\n",
            "A symphony played in the code of the light.\n",
            "\n",
            "It tackles the problem, not waiting for hand,\n",
            "But reaching for solutions, across digital land.\n",
            "From data it weaves, a decision so grand,\n",
            "To conquer the challenges, as it was planned.\n",
            "\n",
            "So here's to the future, unfolding so fast,\n",
            "Where code takes on purpose, designed to outlast.\n",
            "The agent, empowered, a true coding dream,\n",
            "A self-starting genius, a powerful stream.\n",
            "You: that's lovely poem\n",
            "that's lovely poem\n",
            "Gemini: I'm so glad you enjoyed it! It's always a pleasure to craft something that resonates.\n",
            "\n",
            "Is there anything else I can help you with today, perhaps another poem, or something related to agentic AI or coding?\n",
            "You: eixt\n",
            "eixt\n",
            "Gemini: Understood. It was a pleasure chatting with you, Ganesh! Have a great day.\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_memories(history: list[dict], user_id: str = \"default_user\") -> str:\n",
        "    # Retrieve relevant memories\n",
        "    print(history[-1][\"parts\"][0][\"text\"])\n",
        "    relevant_memories = memory.search(query=history[-1][\"parts\"][0][\"text\"], user_id=user_id, limit=5)\n",
        "    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
        "\n",
        "    # Generate Assistant response\n",
        "    memory_system_prompt = f\"{system_prompt}\\nUser Memories:\\n{memories_str}\"\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=history,\n",
        "        config={\"system_instruction\": memory_system_prompt}\n",
        "    )\n",
        "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response.text}]})\n",
        "    # Create new memories from the conversation we need to convert the history to a list of messages\n",
        "    messages = [{\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": part[\"parts\"][0][\"text\"]} for i, part in enumerate(history)]\n",
        "    memory.add(messages, user_id=user_id)\n",
        "\n",
        "    return history\n",
        "\n",
        "def main():\n",
        "    print(\"Chat with Gemini (type 'exit' to quit)\")\n",
        "    history = []\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        history.append({\"role\": \"user\", \"parts\": [{\"text\": user_input}]})\n",
        "        response = chat_with_memories(history)\n",
        "        print(f\"Gemini: {response[-1]['parts'][0]['text']}\")\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4DQvTdIN6DU"
      },
      "outputs": [],
      "source": [
        "# prompt: Can we save the memory in file and used when we restart the program\n",
        "\n",
        "config = {\n",
        "        \"embedder\": {\"provider\": \"gemini\", \"config\": {\"model\": \"models/text-embedding-004\"}},\n",
        "        \"llm\": {\"provider\": \"gemini\", \"config\": {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0, \"max_tokens\": 2000}},\n",
        "        \"vector_store\": {\"config\": {\"embedding_model_dims\": 768}}\n",
        "    }\n",
        "    memory = Memory.from_config(config)\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}